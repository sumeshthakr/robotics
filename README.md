# âš¾ Baseball Orientation Detection

> **Detect 3D orientation of a baseball from monocular video using seam detection and ellipse-based orientation estimation.**

[![CI](https://github.com/sumeshthakr/robotics/actions/workflows/ci.yml/badge.svg)](https://github.com/sumeshthakr/robotics/actions/workflows/ci.yml)
![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8%2B-green)
![YOLOv8](https://img.shields.io/badge/YOLOv8-nano-orange)
![Tests](https://img.shields.io/badge/tests-29%20passed-brightgreen)

---

## ðŸŽ¯ What This Does

Given a 30 fps monocular video of a hand-tossed baseball, the system outputs per-frame:
- **Bounding box** â€” ball location detected by YOLOv8
- **Seam pixels** â€” detected red stitching in the ball ROI
- **Orientation** â€” rotation matrix / quaternion / Euler angles from seam distribution
- **3D trajectory** â€” reconstructed ball path from pinhole camera geometry

### How Orientation is Estimated

| Step | Description |
|------|-------------|
| 1. Detect ball | YOLOv8 nano finds the baseball bounding box |
| 2. Extract seam pixels | Canny edge detection + HSV red color filtering (with circular mask) |
| 3. Fit ellipse | OpenCV `fitEllipse()` on the seam pixel distribution |
| 4. Compute orientation | Ellipse angle â†’ seam direction, axis ratio â†’ seam tilt |
| 5. Build rotation matrix | R = Rz(seam_angle) Ã— Rx(tilt) |

This gives us 2 of 3 rotation degrees of freedom from a single frame. The seam direction tells us which way the stitching runs across the ball, and the tilt tells us how much the seam plane is angled toward or away from the camera.

---

## ðŸ–¼ï¸ Detection Results

### Seam-Based Pipeline â€” Best Frames

| Video 1 | Video 2 |
|:---:|:---:|
| ![Seam Video 1](docs/frames/video1_seam_best.jpg) | ![Seam Video 2](docs/frames/video2_seam_best.jpg) |
| *Red dots = detected seam pixels. Green box = YOLO detection. Euler angles show orientation.* | *Seam pixels detected with ellipse-fitted orientation.* |

---

## ðŸ“ 3D Trajectory Reconstruction

Ball 3D position is recovered from the bounding box using the pinhole camera model:
- **Depth:** `Z = fx Ã— D_real / D_pixel` (ball diameter = 74 mm)
- **Lateral:** `X = (cx_img âˆ’ cx0) Ã— Z / fx`
- **Vertical:** `Y = (cy_img âˆ’ cy0) Ã— Z / fy`

### Detected Ball Path (from bounding box geometry)

| Video 1 â€” 3D Trajectory | Video 2 â€” 3D Trajectory |
|:---:|:---:|
| ![Video 1 Path](docs/frames/video1_detected_path.png) | ![Video 2 Path](docs/frames/video2_detected_path.png) |

### Seam-Based Orientation Arrows

| Video 1 â€” Seam Orientation | Video 2 â€” Seam Orientation |
|:---:|:---:|
| ![Video 1 Seam 3D](docs/frames/video1_seam_orientation.png) | ![Video 2 Seam 3D](docs/frames/video2_seam_orientation.png) |

---

## ðŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BASEBALL ORIENTATION DETECTION                       â”‚
â”‚                     Monocular Video â†’ 3D Orientation + Trajectory           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Input: Video Frame (1700Ã—1200 BGR, 30 fps)
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                   PREPROCESSING STAGE                     â”‚
  â”‚  camera.py                                                â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚ load_camera_params() â†’ K (3Ã—3), dist (1Ã—5)         â”‚  â”‚
  â”‚  â”‚ undistort()          â†’ Remove lens distortion       â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Undistorted frame
                         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                    DETECTION STAGE                        â”‚
  â”‚  detector.py                                              â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚ BallDetector                                        â”‚  â”‚
  â”‚  â”‚  â€¢ YOLOv8n (6M params, COCO pre-trained)            â”‚  â”‚
  â”‚  â”‚  â€¢ Filter: class 32 ("sports ball"), conf â‰¥ 0.25    â”‚  â”‚
  â”‚  â”‚  â€¢ Output: bbox (x1,y1,x2,y2) + confidence         â”‚  â”‚
  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
  â”‚  â”‚ BallTracker                                         â”‚  â”‚
  â”‚  â”‚  â€¢ EMA velocity smoothing (Î±=0.3)                   â”‚  â”‚
  â”‚  â”‚  â€¢ Predict position during lost frames (â‰¤5 frames)  â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ bbox + confidence
                         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                SEAM-BASED PIPELINE                        â”‚
  â”‚  seam_pipeline.py                                         â”‚
  â”‚                                                           â”‚
  â”‚  1. Crop ROI from bbox                                    â”‚
  â”‚  2. Create circular mask (inner 85% of ball)              â”‚
  â”‚  3. Boost HSV saturation (Ã—1.5 for pale seams)            â”‚
  â”‚  4. Canny edge detection (adaptive thresholds)            â”‚
  â”‚  5. HSV dual-range red filter:                            â”‚
  â”‚     hue âˆˆ [0,25] âˆª [155,180]                              â”‚
  â”‚  6. Combine: edges âˆ© red mask                             â”‚
  â”‚  7. Morphological cleanup + dilate                        â”‚
  â”‚                                                           â”‚
  â”‚  ORIENTATION:                                             â”‚
  â”‚  8. Fit ellipse to seam pixel distribution                â”‚
  â”‚  9. Seam angle = ellipse rotation angle                   â”‚
  â”‚  10. Seam tilt = arccos(minor/major axis ratio)           â”‚
  â”‚  11. R = Rz(angle) Ã— Rx(tilt)                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                    OUTPUT STAGE                             â”‚
  â”‚  orientation.py                                            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚ rotation_to_quaternion(R) â†’ [w, x, y, z]            â”‚  â”‚
  â”‚  â”‚ rotation_to_euler(R)      â†’ [roll, pitch, yaw] rad  â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
  Per-frame result dict:
    ball_detected, bbox, confidence, tracking
    orientation { rotation_matrix, quaternion, euler_angles,
                  seam_angle_deg, seam_tilt_deg }
    seam_pixels (Nx2), num_seam_pixels
```

---

## ðŸš€ Getting Started

### Prerequisites

- Python 3.10 or higher
- pip package manager

### Installation

```bash
# Clone the repository
git clone https://github.com/sumeshthakr/robotics.git
cd robotics

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### Running the Pipeline

```bash
# Process a video with visualization
python main.py spin_dataset/raw_spin_video_695d23c184c2b7ababb57a8e_1767711685.mp4 \
    --visualize --output outputs/video1

# Custom confidence threshold
python main.py video.mp4 --confidence 0.3 --output results/
```

### Generating Documentation Outputs

```bash
# Extract best detection frames for documentation
python extract_frames.py

# Generate 3D trajectory and orientation plots
python plot_3d.py
```

### Running Tests & Verification

```bash
# All 29 unit tests
pytest test_all.py -v

# Physical constraint verification (no videos needed)
python verify.py --quick
```

---

## ðŸ—‚ï¸ Project Structure

```
robotics/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml              # CI/CD: lint â†’ test â†’ verify (Python 3.10/3.11)
â”‚
â”œâ”€â”€ camera.py                   # Camera calibration loading + undistortion
â”œâ”€â”€ detector.py                 # YOLOv8 ball detection + EMA velocity tracking
â”œâ”€â”€ orientation.py              # Quaternion/Euler conversion utilities
â”œâ”€â”€ seam_pipeline.py            # Seam detection + ellipse-based orientation
â”œâ”€â”€ main.py                     # CLI entry point
â”œâ”€â”€ extract_frames.py           # Best-frame extractor for documentation
â”œâ”€â”€ plot_3d.py                  # 3D trajectory + orientation visualization
â”œâ”€â”€ test_all.py                 # 29 unit tests (pytest)
â”œâ”€â”€ verify.py                   # Physical-constraint verification
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ camera.json             # fx=10248, dist coeffs, img_shape=(1700,1200,3)
â”‚
â”œâ”€â”€ spin_dataset/               # Two 30 fps spin demo videos
â”‚   â”œâ”€â”€ raw_spin_video_â€¦_1767711685.mp4   (98 frames â€” Video 1)
â”‚   â””â”€â”€ raw_spin_video_â€¦_1767742221.mp4   (85 frames â€” Video 2)
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ frames/                 # Generated detection frames + 3D plots
â”‚       â”œâ”€â”€ video1_seam_best.jpg
â”‚       â”œâ”€â”€ video1_detected_path.png
â”‚       â”œâ”€â”€ video1_seam_orientation.png
â”‚       â”œâ”€â”€ video2_seam_best.jpg
â”‚       â”œâ”€â”€ video2_detected_path.png
â”‚       â”œâ”€â”€ video2_seam_orientation.png
â”‚       â””â”€â”€ metrics.json
â”‚
â”œâ”€â”€ REPORT.md                   # Assignment report (system design + math)
â”œâ”€â”€ AI_COLLABORATION_LOG.md     # AI usage documentation
â”œâ”€â”€ requirements.txt            # ultralytics, opencv-python, numpy, scipy, matplotlib
â””â”€â”€ yolov8n.pt                  # YOLOv8 nano weights (COCO pre-trained)
```

---

## ðŸ” CI/CD Pipeline

The repository uses **GitHub Actions** for continuous integration on every push and pull request.

```
.github/workflows/ci.yml
â”œâ”€â”€ Job: lint-and-test  (Python 3.10 + 3.11 matrix)
â”‚   â”œâ”€â”€ pip install -r requirements.txt + flake8 + pytest
â”‚   â”œâ”€â”€ flake8 (syntax errors & undefined names â†’ fail; style â†’ warn)
â”‚   â””â”€â”€ pytest test_all.py -v  (29 unit tests)
â”‚
â””â”€â”€ Job: quick-verify  (runs after lint-and-test)
    â””â”€â”€ python verify.py --quick  (math/model sanity checks, no video needed)
```

**What's tested in CI (29 tests across 5 modules):**

| Module | Tests | What's Validated |
|---|:---:|---|
| `camera.py` | 3 | JSON loading, missing file error, undistort shape preservation |
| `detector.py` | 9 | Init, confidence, output structure, input validation, tracking |
| `seam_pipeline.py` | 11 | Seam detection, 3D seam model geometry, orientation estimation (PCA/ellipse), pipeline init/reset |
| `orientation.py` | 3 | Quaternion identity, Euler identity, 90Â° rotation conversion |
| Integration | 3 | Pipeline frame processing, video not found, reset |

---

## ðŸ“‹ Requirements

```
ultralytics>=8.0.0     # YOLOv8 ball detection (COCO pre-trained)
opencv-python>=4.8.0   # Image processing, edge detection, ellipse fitting
numpy>=1.24.0          # Array math, linear algebra
scipy>=1.10.0          # Rotation math (scipy.spatial.transform.Rotation)
matplotlib>=3.7.0      # 3D visualization (plot_3d.py)
```

Python 3.10+ required. Tested on Python 3.10 and 3.11.

---

## ðŸ“„ Deliverables

1. **System Design Document** â†’ [`REPORT.md`](REPORT.md) (exposure time, focal length, Hough vs YOLO, bullet spin)
2. **Prototype Code** â†’ This repository (seam-based orientation pipeline)
3. **3D Visualizations** â†’ [`docs/frames/`](docs/frames/) (trajectory plots, orientation arrows)
4. **AI Usage Report** â†’ [`AI_COLLABORATION_LOG.md`](AI_COLLABORATION_LOG.md)
5. **CI/CD Pipeline** â†’ [`.github/workflows/ci.yml`](.github/workflows/ci.yml)
